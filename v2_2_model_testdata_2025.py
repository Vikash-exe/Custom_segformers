# -*- coding: utf-8 -*-
"""V2.2_model_testData_2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gixs3xMi3yRpWd9DeMpxD24ggp-bb_xX
"""

# If needed, uncomment to install deps
# !pip install -q transformers timm tifffile

import os
import re
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
import tifffile as tiff
from tqdm import tqdm
import matplotlib.pyplot as plt
from transformers import SegformerForSemanticSegmentation

# Colab: mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to your zip file
zip_path = "/content/drive/MyDrive/Test_Dataset_2025/Dataset.zip"
extract_dir = "/content"   # where you want to unzip

# Make sure output dir exists
os.makedirs(extract_dir, exist_ok=True)

# Extract zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"✅ Dataset extracted to: {extract_dir}")

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import tifffile as tiff
from torchvision import transforms

# ==============================
# Safe image loader for TIFF / PNG / JPG
# ==============================
def safe_open_tiff(path):
    """
    Opens an image from path and returns a PIL Image in RGB.
    Handles grayscale or multi-channel TIFF images.
    """
    img = tiff.imread(path) if path.lower().endswith((".tif", ".tiff")) else np.array(Image.open(path))

    # Ensure 3 channels
    if img.ndim == 2:  # grayscale → RGB
        img = np.stack([img]*3, axis=-1)
    elif img.shape[2] > 3:  # extra channels → take first 3
        img = img[:, :, :3]

    img = np.clip(img, 0, 255).astype(np.uint8)
    return Image.fromarray(img)

# ==============================
# Inference Dataset
# ==============================
class InferenceDataset(Dataset):
    def __init__(self, img_dir, image_size=512, normalize=True):
        self.img_dir = img_dir
        self.image_files = sorted([
            f for f in os.listdir(img_dir)
            if f.lower().endswith((".png", ".jpg", ".jpeg", ".tif", ".tiff"))
        ])
        self.image_size = image_size
        self.normalize = normalize

        # Define transform
        self.transform = transforms.Compose([transforms.Resize((image_size, image_size)),
                                             transforms.ToTensor()])
        if normalize:
            # ImageNet mean/std
            self.transform.transforms.append(
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            )

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.image_files[idx])
        img = safe_open_tiff(img_path)
        img_tensor = self.transform(img)
        return img_tensor, self.image_files[idx]

# ==============================
# Example: Loading dataset and dataloader
# ==============================
IMG_DIR = "/content/"
dataset = InferenceDataset(IMG_DIR, image_size=512, normalize=True)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

print("✅ Dataset ready. Total images:", len(dataset))

# ==============================
# Inference Example
# ==============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.eval()  # make sure model is in eval mode

for imgs, filenames in dataloader:
    imgs = imgs.to(device)
    with torch.no_grad():
        logits = model(imgs)  # (B, num_classes, H, W)
        preds = torch.argmax(logits, dim=1)  # (B, H, W)

    print(f"Processed {filenames[0]} - Pred shape: {preds.shape} - Classes: {preds.unique()}")

import torch
import torch.nn as nn
from transformers import SegformerModel

class CustomSegFormer(nn.Module):
    def __init__(self, num_classes=5, id2label=None, label2id=None):
        super(CustomSegFormer, self).__init__()

        # === Encoder (pretrained SegFormer B2) ===
        self.encoder = SegformerModel.from_pretrained(
            "nvidia/segformer-b2-finetuned-ade-512-512",
            output_hidden_states=True
        )

        # SegFormer B2 hidden sizes
        embed_dim = [64, 128, 320, 512]
        decoder_dim = 512

        # Projections
        self.linear_c4 = nn.Conv2d(embed_dim[3], decoder_dim, kernel_size=1)
        self.linear_c3 = nn.Conv2d(embed_dim[2], decoder_dim, kernel_size=1)
        self.linear_c2 = nn.Conv2d(embed_dim[1], decoder_dim, kernel_size=1)
        self.linear_c1 = nn.Conv2d(embed_dim[0], decoder_dim, kernel_size=1)

        # Attention-guided upsampling blocks
        self.agu3 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu2 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu1 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)

        # Final classifier
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)

    def forward(self, pixel_values):
        # === Encoder ===
        outputs = self.encoder(pixel_values)
        c1, c2, c3, c4 = outputs.hidden_states  # already (B, C, H, W)

        # Project to same decoder_dim
        c1, c2, c3, c4 = self.linear_c1(c1), self.linear_c2(c2), self.linear_c3(c3), self.linear_c4(c4)

        # === Decoder with AGU ===
        x = self.agu3(c4, c3)  # fuse high-level with C3
        x = self.agu2(x, c2)   # fuse with C2
        x = self.agu1(x, c1)   # fuse with C1

        # Final head
        x = self.dropout(x)
        logits = self.classifier(x)

        # Restore to original image size
        logits = nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode="bilinear", align_corners=False)

        return logits

import torch
import torch.nn as nn

class AttentionUpsample(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()

        # --- Learnable upsampling instead of bilinear ---
        self.up = nn.ConvTranspose2d(
            in_channels, in_channels, kernel_size=2, stride=2
        )

        # Project input and skip connections to same channels
        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv_skip = nn.Conv2d(skip_channels, out_channels, kernel_size=1)

        # Attention map
        self.attn = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # Final refinement
        self.conv_out = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x, skip):
        # 1. Upsample using ConvTranspose2d
        x = self.up(x)

        # 2. If size mismatch (odd dims), force match
        if x.shape[-2:] != skip.shape[-2:]:
            x = nn.functional.interpolate(x, size=skip.shape[2:], mode="nearest")

        # 3. Project features
        x_proj = self.conv_in(x)
        skip_proj = self.conv_skip(skip)

        # 4. Compute attention mask
        attn_map = self.attn(torch.cat([x_proj, skip_proj], dim=1))

        # 5. Weighted fusion
        out = attn_map * x_proj + (1 - attn_map) * skip_proj
        out = self.conv_out(out)
        return out

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.eval()  # set model to evaluation mode

for imgs, filenames in dataloader:
    imgs = imgs.to(device)
    with torch.no_grad():
        logits = model(imgs)  # (B, num_classes, H, W)
        preds = torch.argmax(logits, dim=1)  # (B, H, W)

    print(f"Processed {filenames[0]} - Pred shape: {preds.shape} - Classes: {preds.unique()}")

import tifffile
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image

def run_inference(model, img_path, device="cuda"):
    """
    Runs inference on a single TIFF/PNG/JPG image using a trained CustomSegFormer model.
    Returns the predicted class mask and a colorized mask for visualization.
    """
    # =========================
    # 1. Load image safely
    # =========================
    img_np = tifffile.imread(img_path) if img_path.lower().endswith((".tif", ".tiff")) else np.array(Image.open(img_path))

    # Convert single channel → RGB
    if img_np.ndim == 2:
        img_np = np.stack([img_np]*3, axis=-1)
    # Take first 3 channels if more than 3
    elif img_np.shape[-1] > 3:
        img_np = img_np[..., :3]

    # Resize to 512x512
    pil_img = Image.fromarray(img_np.astype(np.uint8)).resize((512, 512))
    img_np = np.array(pil_img).astype(np.float32) / 255.0  # shape: (H,W,C)

    # =========================
    # 2. Normalize (ImageNet)
    # =========================
    # Use channels-last broadcasting to avoid ValueError
    mean = np.array([0.485, 0.456, 0.406])
    std  = np.array([0.229, 0.224, 0.225])
    img_np = (img_np - mean) / std  # broadcasting works over last channel

    # Convert to tensor (B,C,H,W)
    img_tensor = torch.tensor(img_np).permute(2,0,1).unsqueeze(0).float().to(device)

    # =========================
    # 3. Run model
    # =========================
    model.eval()
    with torch.no_grad():
        logits = model(img_tensor)
        pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()

    # =========================
    # 4. Apply colormap for visualization
    # =========================
    colors = np.array([
        [255, 255, 0],   # class 0
        [255, 0, 0],     # class 1
        [0, 0, 255],     # class 2
        [0, 255, 0],     # class 3
        [128, 128, 128], # class 4
    ], dtype=np.uint8)
    pred_colored = colors[pred]

    # =========================
    # 5. Display results
    # =========================
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.imshow(pil_img)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1,2,2)
    plt.imshow(pred_colored)
    plt.title("Predicted Mask")
    plt.axis("off")
    plt.show()

    return pred, pred_colored

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

missing_keys, unexpected_keys = model.load_state_dict(
    torch.load("/content/drive/MyDrive/2.2_CustomB2_CE+Lovasz_lulc/model_state_dict.pth"), strict=False
)
print("Missing keys:", missing_keys)
print("Unexpected keys:", unexpected_keys)

pred_mask, pred_colored = run_inference(model, "/content/Image_055.tif", device="cuda")