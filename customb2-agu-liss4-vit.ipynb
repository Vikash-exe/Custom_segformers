{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12946916,"sourceType":"datasetVersion","datasetId":8193148},{"sourceId":12956379,"sourceType":"datasetVersion","datasetId":8199811}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# If needed, uncomment to install deps\n# !pip install -q transformers timm tifffile\n\nimport os\nimport re\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport tifffile as tiff\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom transformers import SegformerForSemanticSegmentation\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:05:54.410636Z","iopub.execute_input":"2025-09-04T07:05:54.410796Z","iopub.status.idle":"2025-09-04T07:06:19.367065Z","shell.execute_reply.started":"2025-09-04T07:05:54.410781Z","shell.execute_reply":"2025-09-04T07:06:19.366494Z"},"_kg_hide-output":false},"outputs":[{"name":"stderr","text":"2025-09-04 07:06:07.776318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756969567.972814      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756969568.031640      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Dataset name: Sorted_3years_LULC","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(\"Datasets under /kaggle/input/:\")\nprint(os.listdir(\"/kaggle/input/\"))\n\nprint(\"\\nContents of your dataset folder:\")\nprint(os.listdir(\"/kaggle/input/lulc-dataset\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:08:59.940817Z","iopub.execute_input":"2025-09-04T07:08:59.941355Z","iopub.status.idle":"2025-09-04T07:08:59.948164Z","shell.execute_reply.started":"2025-09-04T07:08:59.941328Z","shell.execute_reply":"2025-09-04T07:08:59.947582Z"}},"outputs":[{"name":"stdout","text":"Datasets under /kaggle/input/:\n['lulc-dataset']\n\nContents of your dataset folder:\n['3yeardata']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\nroot_dir = \"/kaggle/input/lulc-dataset/3yeardata\"\nprint(\"Root contents:\", os.listdir(root_dir))\n# Check for subfolders\nif \"Images\" in os.listdir(root_dir):\n    print(\"Images folder contents:\", os.listdir(os.path.join(root_dir, \"Images\"))[:10])\nif \"Mask\" in os.listdir(root_dir):\n    print(\"Mask folder contents:\", os.listdir(os.path.join(root_dir,\"Mask\"))[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:14:58.338994Z","iopub.execute_input":"2025-09-04T07:14:58.339606Z","iopub.status.idle":"2025-09-04T07:14:58.370145Z","shell.execute_reply.started":"2025-09-04T07:14:58.339580Z","shell.execute_reply":"2025-09-04T07:14:58.369435Z"}},"outputs":[{"name":"stdout","text":"Root contents: ['Images', 'Mask']\nImages folder contents: ['Image_191.tif', 'Image_178.tif', 'Image_014.tif', 'Image_103.tif', 'Image_140.tif', 'Image_091.tif', 'Image_157.tif', 'Image_108.tif', 'Image_022.tif', 'Image_077.tif']\nMask folder contents: ['Mask_231.tif', 'Mask_073.tif', 'Mask_064.tif', 'Mask_036.tif', 'Mask_103.tif', 'Mask_065.tif', 'Mask_049.tif', 'Mask_143.tif', 'Mask_148.tif', 'Mask_141.tif']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install -q transformers datasets huggingface_hub --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:24:17.648144Z","iopub.execute_input":"2025-09-04T07:24:17.648942Z","iopub.status.idle":"2025-09-04T07:24:19.086873Z","shell.execute_reply.started":"2025-09-04T07:24:17.648912Z","shell.execute_reply":"2025-09-04T07:24:19.086021Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass LULCDataset(Dataset):\n    def __init__(self, images_dir, masks_dir, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n\n        # Match files by sorting\n        self.image_files = sorted(os.listdir(images_dir))\n        self.mask_files  = sorted(os.listdir(masks_dir))\n\n        assert len(self.image_files) == len(self.mask_files), \\\n            \"❌ Number of images and masks do not match!\"\n\n        self.pairs = list(zip(self.image_files, self.mask_files))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.images_dir, self.pairs[idx][0])\n        mask_path = os.path.join(self.masks_dir, self.pairs[idx][1])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        mask  = Image.open(mask_path).convert(\"L\")  # grayscale\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:24:23.351677Z","iopub.execute_input":"2025-09-04T07:24:23.352369Z","iopub.status.idle":"2025-09-04T07:24:23.358164Z","shell.execute_reply.started":"2025-09-04T07:24:23.352343Z","shell.execute_reply":"2025-09-04T07:24:23.357410Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from transformers import SegformerImageProcessor\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n\ndataset = LULCDataset(IMAGES_DIR, MASKS_DIR, transform=processor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:16.614976Z","iopub.execute_input":"2025-09-04T07:28:16.615282Z","iopub.status.idle":"2025-09-04T07:28:16.876896Z","shell.execute_reply.started":"2025-09-04T07:28:16.615260Z","shell.execute_reply":"2025-09-04T07:28:16.876166Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e7d2c173314f479160fae0f091c1ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e3747a1a364efaac97cd24f4db9220"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_base.py:410: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n  image_processor = cls(**image_processor_dict)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"IMAGES_DIR = \"/kaggle/input/lulc-dataset/3yeardata/Images\"\nMASKS_DIR  = \"/kaggle/input/lulc-dataset/3yeardata/Mask\"\n\ndataset = LULCDataset(IMAGES_DIR, MASKS_DIR, processor)\n\nprint(f\"✅ Dataset loaded with {len(dataset)} pairs\")\n\n# Show first 5 pairs to confirm correct matching\nfor i in range(5):\n    img_file, mask_file = dataset.pairs[i]\n    print(f\"Pair {i}: {img_file}  <->  {mask_file}\")\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"✅ Dataset loaded with 232 pairs\nPair 0: Image_001.tif  <->  Mask_001.tif\nPair 1: Image_002.tif  <->  Mask_002.tif\nPair 2: Image_003.tif  <->  Mask_003.tif\nPair 3: Image_004.tif  <->  Mask_004.tif\nPair 4: Image_005.tif  <->  Mask_005.tif\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, images_dir, masks_dir, image_size=512, num_classes=4, normalize=True):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.image_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.tif','.tiff','.png','.jpg','.jpeg'))])\n        self.mask_files  = sorted([f for f in os.listdir(masks_dir)  if f.lower().endswith(('.tif','.tiff','.png','.jpg','.jpeg'))])\n\n        assert len(self.image_files) == len(self.mask_files), f\"Images({len(self.image_files)}) != Masks({len(self.mask_files)})\"\n        self.image_size = image_size\n        self.num_classes = num_classes\n        self.normalize = normalize\n        # ImageNet mean/std (SegFormer preproc defaults)\n        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3,1,1)\n        self.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3,1,1)\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def _read_image(self, path):\n        if path.lower().endswith(('.tif','.tiff')):\n            arr = tiff.imread(path)          # may be float, extra dims\n        else:\n            arr = np.array(Image.open(path)) # HWC\n\n        arr = np.squeeze(arr)\n        # Scale non-uint8 to 0..255\n        if arr.dtype != np.uint8:\n            arr = (255 * (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)).astype(np.uint8)\n        # Grayscale → RGB\n        if arr.ndim == 2:\n            arr = np.stack([arr]*3, axis=-1)\n        return arr  # HWC uint8\n\n    def _read_mask(self, path):\n        if path.lower().endswith(('.tif','.tiff')):\n            arr = tiff.imread(path)\n        else:\n            arr = np.array(Image.open(path))\n        arr = np.squeeze(arr)\n        if arr.ndim == 3:\n            arr = arr[:, :, 0]\n        return arr  # HW (integers)\n\n    def __getitem__(self, idx):\n        img_path  = os.path.join(self.images_dir, self.image_files[idx])\n        mask_path = os.path.join(self.masks_dir,  self.mask_files[idx])\n\n        # ---- Image ----\n        img = self._read_image(img_path)\n        img = Image.fromarray(img).resize((self.image_size, self.image_size), resample=Image.BILINEAR)\n        img = np.array(img, dtype=np.float32) / 255.0        # [0,1]\n        img = np.transpose(img, (2, 0, 1))                   # C,H,W\n        if self.normalize:\n            img = (img - self.mean) / self.std\n\n        # ---- Mask ----\n        mask = self._read_mask(mask_path)\n        mask = Image.fromarray(mask).resize((self.image_size, self.image_size), resample=Image.NEAREST)\n        mask = np.array(mask, dtype=np.int64)\n        # Remap anything >=4 to class 3 (\"Others\")\n        mask[mask >= self.num_classes] = self.num_classes - 1\n\n        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:27.920653Z","iopub.execute_input":"2025-09-04T07:28:27.920934Z","iopub.status.idle":"2025-09-04T07:28:27.934062Z","shell.execute_reply.started":"2025-09-04T07:28:27.920912Z","shell.execute_reply":"2025-09-04T07:28:27.933499Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"full_dataset = SegmentationDataset(IMAGES_DIR, MASKS_DIR, image_size=512, num_classes=4)\n\n# 80/20 split\ntrain_size = int(0.8 * len(full_dataset))\nval_size   = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\nBATCH_SIZE = 4\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\nxb, yb = next(iter(train_loader))\nprint(\"Batch shapes -> images:\", tuple(xb.shape), \"masks:\", tuple(yb.shape))  # expect [B,3,512,512], [B,512,512]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:31.633712Z","iopub.execute_input":"2025-09-04T07:28:31.633975Z","iopub.status.idle":"2025-09-04T07:28:32.300478Z","shell.execute_reply.started":"2025-09-04T07:28:31.633956Z","shell.execute_reply":"2025-09-04T07:28:32.299629Z"}},"outputs":[{"name":"stdout","text":"Train samples: 185, Val samples: 47\nBatch shapes -> images: (4, 3, 512, 512) masks: (4, 512, 512)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"class AttentionUpsample(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        # Project input and skip connections\n        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.conv_skip = nn.Conv2d(skip_channels, out_channels, kernel_size=1)\n\n        # Attention map\n        self.attn = nn.Sequential(\n            nn.Conv2d(out_channels * 2, out_channels, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n        self.conv_out = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x, skip):\n        # 1. Align size\n        x = nn.functional.interpolate(x, size=skip.shape[2:], mode=\"bilinear\", align_corners=False)\n\n        # 2. Project features\n        x_proj = self.conv_in(x)\n        skip_proj = self.conv_skip(skip)\n\n        # 3. Compute attention mask\n        attn_map = self.attn(torch.cat([x_proj, skip_proj], dim=1))\n\n        # 4. Weighted fusion\n        out = attn_map * x_proj + (1 - attn_map) * skip_proj\n        out = self.conv_out(out)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:35.184278Z","iopub.execute_input":"2025-09-04T07:28:35.184576Z","iopub.status.idle":"2025-09-04T07:28:35.191537Z","shell.execute_reply.started":"2025-09-04T07:28:35.184545Z","shell.execute_reply":"2025-09-04T07:28:35.190781Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import SegformerModel\n\nclass CustomSegFormer(nn.Module):\n    def __init__(self, num_classes=4, id2label=None, label2id=None):\n        super(CustomSegFormer, self).__init__()\n\n        # === Encoder (pretrained SegFormer B2) ===\n        self.encoder = SegformerModel.from_pretrained(\n            \"nvidia/segformer-b2-finetuned-ade-512-512\",\n            output_hidden_states=True\n        )\n\n        # SegFormer B2 hidden sizes\n        embed_dim = [64, 128, 320, 512]\n        decoder_dim = 256\n\n        # Projections\n        self.linear_c4 = nn.Conv2d(embed_dim[3], decoder_dim, kernel_size=1)\n        self.linear_c3 = nn.Conv2d(embed_dim[2], decoder_dim, kernel_size=1)\n        self.linear_c2 = nn.Conv2d(embed_dim[1], decoder_dim, kernel_size=1)\n        self.linear_c1 = nn.Conv2d(embed_dim[0], decoder_dim, kernel_size=1)\n\n        # Attention-guided upsampling blocks\n        self.agu3 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)\n        self.agu2 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)\n        self.agu1 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)\n\n        # Final classifier\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)\n\n    def forward(self, pixel_values):\n        # === Encoder ===\n        outputs = self.encoder(pixel_values)\n        c1, c2, c3, c4 = outputs.hidden_states  # already (B, C, H, W)\n\n        # Project to same decoder_dim\n        c1, c2, c3, c4 = self.linear_c1(c1), self.linear_c2(c2), self.linear_c3(c3), self.linear_c4(c4)\n\n        # === Decoder with AGU ===\n        x = self.agu3(c4, c3)  # fuse high-level with C3\n        x = self.agu2(x, c2)   # fuse with C2\n        x = self.agu1(x, c1)   # fuse with C1\n\n        # Final head\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        # Restore to original image size\n        logits = nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:39.672959Z","iopub.execute_input":"2025-09-04T07:28:39.673246Z","iopub.status.idle":"2025-09-04T07:28:39.681058Z","shell.execute_reply.started":"2025-09-04T07:28:39.673225Z","shell.execute_reply":"2025-09-04T07:28:39.680321Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"✅ GPU is available!\")\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"❌ No GPU detected.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:44.705385Z","iopub.execute_input":"2025-09-04T07:28:44.706036Z","iopub.status.idle":"2025-09-04T07:28:44.710348Z","shell.execute_reply.started":"2025-09-04T07:28:44.706013Z","shell.execute_reply":"2025-09-04T07:28:44.709577Z"}},"outputs":[{"name":"stdout","text":"✅ GPU is available!\nGPU Name: Tesla T4\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# === Usage ===\nid2label = {0: \"Urban\", 1: \"Water\", 2: \"Vegetation\", 3: \"Others\"}\nlabel2id = {v: k for k, v in id2label.items()}\n\nmodel = CustomSegFormer(num_classes=4, id2label=id2label, label2id=label2id)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:28:53.226165Z","iopub.execute_input":"2025-09-04T07:28:53.226480Z","iopub.status.idle":"2025-09-04T07:28:55.677291Z","shell.execute_reply.started":"2025-09-04T07:28:53.226460Z","shell.execute_reply":"2025-09-04T07:28:55.676532Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1930056339f149a4a46dabf1f3c5fd72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23ac4c3b9bb4ecfa05efcc787b48233"}},"metadata":{}},{"name":"stdout","text":"CustomSegFormer(\n  (encoder): SegformerModel(\n    (encoder): SegformerEncoder(\n      (patch_embeddings): ModuleList(\n        (0): SegformerOverlapPatchEmbeddings(\n          (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): SegformerOverlapPatchEmbeddings(\n          (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): SegformerOverlapPatchEmbeddings(\n          (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): SegformerOverlapPatchEmbeddings(\n          (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (block): ModuleList(\n        (0): ModuleList(\n          (0): SegformerLayer(\n            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=64, out_features=64, bias=True)\n                (key): Linear(in_features=64, out_features=64, bias=True)\n                (value): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): Identity()\n            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=64, out_features=256, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=256, out_features=64, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SegformerLayer(\n            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=64, out_features=64, bias=True)\n                (key): Linear(in_features=64, out_features=64, bias=True)\n                (value): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.006666666828095913)\n            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=64, out_features=256, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=256, out_features=64, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SegformerLayer(\n            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=64, out_features=64, bias=True)\n                (key): Linear(in_features=64, out_features=64, bias=True)\n                (value): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=64, out_features=64, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.013333333656191826)\n            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=64, out_features=256, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=256, out_features=64, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (1): ModuleList(\n          (0): SegformerLayer(\n            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.019999999552965164)\n            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=128, out_features=512, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SegformerLayer(\n            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.02666666731238365)\n            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=128, out_features=512, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SegformerLayer(\n            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.03333333507180214)\n            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=128, out_features=512, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SegformerLayer(\n            (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.03999999910593033)\n            (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=128, out_features=512, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (2): ModuleList(\n          (0): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.046666666865348816)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.0533333346247673)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.06000000238418579)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.06666667014360428)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.07333333790302277)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): SegformerLayer(\n            (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=320, out_features=320, bias=True)\n                (key): Linear(in_features=320, out_features=320, bias=True)\n                (value): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n                (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n                (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=320, out_features=320, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.07999999821186066)\n            (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=320, out_features=1280, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=1280, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (3): ModuleList(\n          (0): SegformerLayer(\n            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=512, out_features=512, bias=True)\n                (key): Linear(in_features=512, out_features=512, bias=True)\n                (value): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.08666666597127914)\n            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=512, out_features=2048, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=2048, out_features=512, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): SegformerLayer(\n            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=512, out_features=512, bias=True)\n                (key): Linear(in_features=512, out_features=512, bias=True)\n                (value): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.09333333373069763)\n            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=512, out_features=2048, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=2048, out_features=512, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): SegformerLayer(\n            (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attention): SegformerAttention(\n              (self): SegformerEfficientSelfAttention(\n                (query): Linear(in_features=512, out_features=512, bias=True)\n                (key): Linear(in_features=512, out_features=512, bias=True)\n                (value): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): SegformerSelfOutput(\n                (dense): Linear(in_features=512, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (drop_path): SegformerDropPath(p=0.10000000149011612)\n            (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): SegformerMixFFN(\n              (dense1): Linear(in_features=512, out_features=2048, bias=True)\n              (dwconv): SegformerDWConv(\n                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n              )\n              (intermediate_act_fn): GELUActivation()\n              (dense2): Linear(in_features=2048, out_features=512, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n      (layer_norm): ModuleList(\n        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (linear_c4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n  (linear_c3): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n  (linear_c2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n  (linear_c1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n  (agu3): AttentionUpsample(\n    (conv_in): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (conv_skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (attn): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n      (3): Sigmoid()\n    )\n    (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (agu2): AttentionUpsample(\n    (conv_in): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (conv_skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (attn): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n      (3): Sigmoid()\n    )\n    (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (agu1): AttentionUpsample(\n    (conv_in): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (conv_skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (attn): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n      (3): Sigmoid()\n    )\n    (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c07a7cecd254b1cb9acc3eb9cb219be"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# id2label = {0:\"Urban\", 1:\"Water\", 2:\"Vegetation\", 3:\"Others\"}\n# label2id = {v:k for k,v in id2label.items()}\n\n# model = SegformerForSemanticSegmentation.from_pretrained(\n#     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n#     num_labels=4,\n#     id2label=id2label,\n#     label2id=label2id,\n#     ignore_mismatched_sizes=True,  # new head (4 classes) initialized\n# )\nmodel.to(device)\n\n# Optim & training settings\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\nnum_epochs = 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:30:51.815416Z","iopub.execute_input":"2025-09-04T07:30:51.815757Z","iopub.status.idle":"2025-09-04T07:30:51.890824Z","shell.execute_reply.started":"2025-09-04T07:30:51.815736Z","shell.execute_reply":"2025-09-04T07:30:51.890075Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def pixel_accuracy(preds, labels):\n    # preds: [B,H,W] (class ids), labels: [B,H,W]\n    correct = (preds == labels).sum().item()\n    total = labels.numel()\n    return correct / total\n\ndef mean_iou(preds, labels, num_classes=4):\n    # preds, labels: [B,H,W]\n    ious = []\n    for cls in range(num_classes):\n        pred_c = (preds == cls)\n        label_c = (labels == cls)\n        inter = (pred_c & label_c).sum().item()\n        union = (pred_c | label_c).sum().item()\n        if union == 0:\n            ious.append(float(\"nan\"))  # ignore if class not present in both\n        else:\n            ious.append(inter / union)\n    return np.nanmean(ious)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:31:15.911150Z","iopub.execute_input":"2025-09-04T07:31:15.911926Z","iopub.status.idle":"2025-09-04T07:31:15.917014Z","shell.execute_reply.started":"2025-09-04T07:31:15.911901Z","shell.execute_reply":"2025-09-04T07:31:15.916146Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(f\"Train samples: {len(train_dataset)} ({len(train_loader)} batches of size {BATCH_SIZE})\")\nprint(f\"Val samples:   {len(val_dataset)} ({len(val_loader)} batches of size {BATCH_SIZE})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:31:26.598265Z","iopub.execute_input":"2025-09-04T07:31:26.598974Z","iopub.status.idle":"2025-09-04T07:31:26.603091Z","shell.execute_reply.started":"2025-09-04T07:31:26.598950Z","shell.execute_reply":"2025-09-04T07:31:26.602466Z"}},"outputs":[{"name":"stdout","text":"Train samples: 185 (47 batches of size 4)\nVal samples:   47 (12 batches of size 4)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"train_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\ntrain_ious, val_ious = [], []\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(1, num_epochs+1):\n    # ---- Train ----\n    model.train()\n    tr_loss, tr_acc, tr_iou = 0.0, 0.0, 0.0\n    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\"):\n        imgs, masks = imgs.to(device), masks.to(device)\n\n        # out = model(pixel_values=imgs, labels=masks)   # CrossEntropy inside\n        # loss = out.loss\n        # logits = out.logits                            # [B, num_classes, H, W]\n\n        logits = model(pixel_values=imgs)             # [B, num_classes, H, W]\n        loss = criterion(logits, masks)               # Calculate loss manually\n\n        preds = torch.argmax(logits, dim=1)            # [B,H,W]\n\n        # Resize predictions to match mask size for metric calculation\n        preds_resized = torch.nn.functional.interpolate(preds.unsqueeze(1).float(),\n                                                        size=masks.shape[-2:],\n                                                        mode='nearest').squeeze(1).long()\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss += loss.item()\n        tr_acc  += pixel_accuracy(preds_resized, masks)\n        tr_iou  += mean_iou(preds_resized, masks, num_classes=4)\n\n    tr_loss /= len(train_loader)\n    tr_acc  /= len(train_loader)\n    tr_iou  /= len(train_loader)\n\n    train_losses.append(tr_loss)\n    train_accs.append(tr_acc)\n    train_ious.append(tr_iou)\n\n    # ---- Validate ----\n    model.eval()\n    va_loss, va_acc, va_iou = 0.0, 0.0, 0.0\n    with torch.no_grad():\n        for imgs, masks in tqdm(val_loader, desc=\"Validating\"):\n            imgs, masks = imgs.to(device), masks.to(device)\n            # out = model(pixel_values=imgs, labels=masks)\n            # va_loss += out.loss.item()\n            logits = model(pixel_values=imgs)\n            va_loss += criterion(logits, masks).item()\n\n\n            # logits = out.logits  # Remove this line\n            preds = torch.argmax(logits, dim=1)\n            # Resize predictions to match mask size for metric calculation\n            preds_resized = torch.nn.functional.interpolate(preds.unsqueeze(1).float(),\n                                                            size=masks.shape[-2:],\n                                                            mode='nearest').squeeze(1).long()\n\n\n            va_acc += pixel_accuracy(preds_resized, masks)\n            va_iou += mean_iou(preds_resized, masks, num_classes=4)\n\n    va_loss /= len(val_loader)\n    va_acc  /= len(val_loader)\n    va_iou  /= len(val_loader)\n\n    val_losses.append(va_loss)\n    val_accs.append(va_acc)\n    val_ious.append(va_iou)\n\n    print(f\"Epoch {epoch}/{num_epochs} | \"\n          f\"Train Loss: {tr_loss:.4f}  Val Loss: {va_loss:.4f}  |  \"\n          f\"Train Acc: {tr_acc*100:.2f}%  Val Acc: {va_acc*100:.2f}%  |  \"\n          f\"Train mIoU: {tr_iou:.4f}  Val mIoU: {va_iou:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T07:32:03.246453Z","iopub.execute_input":"2025-09-04T07:32:03.246729Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10 - Training:  60%|█████▉    | 28/47 [00:16<00:10,  1.84it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, num_epochs+1)\nplt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(epochs, train_losses, label=\"Train\")\nplt.plot(epochs, val_losses, label=\"Val\")\nplt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(epochs, [a*100 for a in train_accs], label=\"Train\")\nplt.plot(epochs, [a*100 for a in val_accs], label=\"Val\")\nplt.title(\"Pixel Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(epochs, train_ious, label=\"Train\")\nplt.plot(epochs, val_ious, label=\"Val\")\nplt.title(\"Mean IoU\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"mIoU\"); plt.legend()\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n# Color map for 5 classes\nCOLOR_MAP = {\n    0: (255, 255,   0),   # Barren -> Yellow\n    1: (255,   0,   0),   # Urban  -> Red\n    2: (  0,   0, 255),   # Water  -> Blue\n    3: (  0, 255,   0),   # Vegetation -> Green\n    4: (128, 128, 128)    # Others -> Gray\n}\n\ndef decode_color(mask_np):\n    \"\"\"Map class indices to RGB colors for visualization\"\"\"\n    h, w = mask_np.shape\n    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n    for cls, color in COLOR_MAP.items():\n        rgb[mask_np == cls] = color\n    return rgb\n\n# -------------------------------\n# Inference for a specific index\n# -------------------------------\nmodel.eval()\nidx = 6   # <-- choose any index here\n\n# Make sure dataset returns correct pair\nimg, true_mask = val_dataset[idx]   # img: [3,512,512], true_mask: [512,512]\nimg_batch = img.unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    outputs = model(pixel_values=img_batch)\n    pred_mask = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy()  # Changed outputs.logits to outputs\n\n# Convert tensors back for visualization\ntrue_mask = true_mask.cpu().numpy()\nimg_vis = img.cpu().numpy()\n\n# Undo normalization for display\nmean = np.array([0.485, 0.456, 0.406]).reshape(3,1,1)\nstd  = np.array([0.229, 0.224, 0.225]).reshape(3,1,1)\nimg_vis = (img_vis * std + mean).clip(0,1)\nimg_vis = np.transpose(img_vis, (1,2,0))  # CHW -> HWC\n\n# -------------------------------\n# Show results\n# -------------------------------\nplt.figure(figsize=(15,5))\nplt.subplot(1,3,1); plt.imshow(img_vis); plt.title(f\"Original Image #{idx}\"); plt.axis(\"off\")\nplt.subplot(1,3,2); plt.imshow(decode_color(true_mask)); plt.title(\"Ground Truth Mask\"); plt.axis(\"off\")\nplt.subplot(1,3,3); plt.imshow(decode_color(pred_mask)); plt.title(\"Predicted Mask\"); plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}